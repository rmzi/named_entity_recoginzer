COMSW4705 - Natural Language Processing
Homework 1 - Ramzi Abdoch, raa214

_Q4_

files:
	- count_freqs.py
	  - added function calc_emissions, per Q4a
	  - added some class variables (all_words, word_counts)
	- replace_rare.py
	  - call "python replace_rare.py [counts_file] [input_file]" to run
	  - ***CAUTION*** script operates on input_file in place
	- tagger.py
	  - call "python tagger.py [counts_file (after calling replace_rare.py)] [test_data_file] > [predictions_file]"

how to run:
	1. python count_freqs.py ner_train.dat > ner.counts
	2. python replace_rare.py ner.counts ner_train.dat
	3. python count_freqs.py ner_train.dat > ner_w_rare.counts
	4. python tagger.py ner_w_rare.counts ner_dev.dat > preds.dat
	5. python eval_ne_tagger.py ner_dev.key preds.dat

results:

	Found 14043 NEs. Expected 5931 NEs; Correct: 3117.

		 precision 	recall 		F1-Score
	Total:	 0.221961	0.525544	0.312106
	PER:	 0.435451	0.231230	0.302061
	ORG:	 0.475936	0.399103	0.434146
	LOC:	 0.147750	0.870229	0.252612
	MISC:	 0.491689	0.610206	0.544574

observations:
  - The vast majority of words (~82%) are _RARE_. The remaining ~18% are the only words we should calculate predictions for because the prediction will be the same for every _RARE_ word. It would be more efficient to cache the _RARE_ probabilities for each class. This naive tagger does not account for positional data from the training set, simply counts. As a result, These results are fairly poor. It also found much more (3x as many) NE than expected.

_Q5_

files:
	- trigram_probs.py
		- call "python trigram_probs.py [counts_file] [input_file] > [output_file]" to run
	- count_freqs.py
		- added calc_mle, per Q5a
	- viterbi.py
		- call "python viterbi [counts_file] [test_file] > [predictions_file]"

how to run:
	1. python viterbi.py ner.counts ner_dev.dat > preds.dat
	2. python eval_ne_tagger.py ner_dev.key preds.dat

results:

	Found 4657 NEs. Expected 5931 NEs; Correct: 3145.

		 precision 	recall 		F1-Score
	Total:	 0.675327	0.530265	0.594069
	PER:	 0.538406	0.404244	0.461778
	ORG:	 0.539554	0.397608	0.457831
	LOC:	 0.850667	0.695747	0.765447
	MISC:	 0.750948	0.644951	0.693925

observations:
  - The scores are much higher with this method. The contextual data greatly aids the classifier in determining the correct tag sequence. It also found less NE than expected, but far less than the previous method.
  - Dynamic programming is challenging to understand, but really powerful! It was a great challenge to put this together.

_Q6_

files: 
	- group_rare.py
		- group words based on three criteria
		  - All caps
		  - First letter capitalized
		  - Numbers + punctuation
		- call "python group_rare.py [counts_file] [training_file]" to run
	- viterbigrouped.py
		- call "python viterbi [counts_file] [test_data] > [predictions_file]" to run

how to run:
 	1. python count_freqs.py ner_train.dat > ner.counts
	2. python group_rare.py ner.counts ner_train.dat
	3. python count_freqs.py ner_train.dat > ner_grouped_rare.counts
	4. python viterbi_grouped.py ner_grouped_rare.counts ner_dev.dat > preds.dat
	
	5. python eval_ne_tagger.py ner_dev.key preds.dat

results:

	Found 6568 NEs. Expected 5931 NEs; Correct: 4190.

		 	precision 	recall 		F1-Score
	Total:	 0.637942	0.706458	0.670454
	PER:	 0.579649	0.898803	0.704778
	ORG:	 0.484397	0.510463	0.497089
	LOC:	 0.837500	0.694111	0.759094
	MISC:	 0.738579	0.631922	0.681100

observations:
  - Grouping the rare words with the above criteria significantly helped the recall and F1-scores of the Viterbi tagger, but the precision scores stayed the same or were slightly lower. This means if a certain word is actually tagged PER, for example, we tagged it correctly more often, but we also mistagged words inaccurately as PER more often as well.